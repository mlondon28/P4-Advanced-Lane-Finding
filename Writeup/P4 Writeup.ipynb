{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# **Finding Lane Lines on the Road** \n",
    "\n",
    "---\n",
    "\n",
    "**Finding Lane Lines on the Road**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Make a pipeline that finds lane lines on the road\n",
    "\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./images/undistort_example.png \"Original Image / Undistorted Image\"\n",
    "[image2]: ./images/hls_binary.png \"HLS Binary\"\n",
    "[image3]: ./images/R_sobels.png \"Sobel Filters on Red Channel\"\n",
    "[image4]: ./images/R_thresh.png \"Red Sobel Threshold\"\n",
    "[image5]: ./images/L_sobel.png \"Luminance Sobel Threshold\"\n",
    "[image6]: ./images/S_sobel.png \"Saturation Sobel Threshold\"\n",
    "[image7]: ./images/ycrcb_binary.png \"YCrCb Binary\"\n",
    "[image8]: ./images/combined.png \"Combined Binary\"\n",
    "[image9]: ./images/histogram.png \"Histogram of Lane Lines\"\n",
    "[image10]: ./images/sliding_window_plt.png \"Sliding Window\"\n",
    "[image11]: ./images/identified_lanes_warped.png \"Identified Lanes\"\n",
    "[image12]: ./images/radius_equation.png \"Radius Equation\"\n",
    "[image13]: ./images/projected_lane.png \"Projected Lane\"\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Reflection\n",
    "\n",
    "## Camera Calibration\n",
    "\n",
    "To calibrate the lens used for all of the images in the project videos I used a few OpenCV techniques and functions to generate the correct camera matrix and distortion coefficients. \n",
    "\n",
    "The process I used was:\n",
    "\n",
    "1) Iterate through images of a chessboard (taken at different angles / distances) taken with the same camera lens used on the dash cam videos\n",
    "\n",
    "2) Find chessboard corners based on the number of x and y inside corners (Used `cv2.findChessboardCorners()`). \n",
    "\n",
    "3) Draw chessboard corners on original image to verify all the corners were found correctly. \n",
    "\n",
    "4) Then using `cv2.calibrateCamera()` to return the camera matrix (`mtx`) and distortion (`dist`) coefficients\n",
    "\n",
    "5) `mtx` and `dist` were then used in conjunction with `cv2.undistort()` to return an undistorted image\n",
    "\n",
    "Once this was done, the camera matrix (mtx) and distortion coefficients (dist) were saved in a pickle so they could be loaded later without having to recalculate.\n",
    "\n",
    "\n",
    "\n",
    "**Here is a test image unedited and then undistorted:**\n",
    "![alt text][image1]\n",
    "\n",
    "```python\n",
    "def undistortImage(img, mtx, dist):\n",
    "    \"\"\"\n",
    "    Removes Lens distortion from RAW image. \n",
    "    Returns undistorted image\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist\n",
    "```\n",
    "\n",
    "\n",
    "## Image Pipeline \n",
    "\n",
    "#### Correcting for Image Distortion\n",
    "\n",
    "The procedure listed above in the camera calibration section was used in the pipeline to correct for lens distortion.\n",
    "\n",
    "#### Creating a Binary Image\n",
    "\n",
    "There are many filters that went into creating the final binary image. The final binary needed to be robust enough to successfully identify lines in rapidly changing light and shadow as well as different colored lane lines and pavement. \n",
    "\n",
    "Color spaces used:\n",
    "- RGB: (Red, Green, Blue). The Red channel was used to help identify yellow lines via a Sobel filter. ![Red Sobels][image3] ![Red combined][image4]\n",
    "- HLS: (Hue, Lightness, Saturation). Lightness and saturation were separated, thresholded and used for the final binary image ![HLS][image2]\n",
    "- Luminance Sobel threshold: HLS Luminance channel was used to help pull out lane lines in various (shaded) lighting conditions. ![Luminance Sobel][image5]\n",
    "- Saturation Sobel Threshold: HLS Saturation channel was used to pull out colored lines in all lighting situations. ![S_sobel][image6]\n",
    "- YCrCb Binary: To make the yellow lane detection in all lighting situations more robust, the Cb channel was used. ![YCrCb Binary][image7]\n",
    "- Finally all channels were combined and masked to yield the final warped binary image. ![Combined Image][image8]\n",
    "\n",
    "The Sobel Theshold functions that were used: \n",
    "\n",
    "```python\n",
    "def abs_sobel_thresh(image, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Calculate directional gradient\n",
    "    # Apply threshold\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(image, cv2.CV_64F, 1, 0))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(image, cv2.CV_64F, 0, 1))\n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Create a copy and apply the threshold\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    # Here I'm using inclusive (>=, <=) thresholds, but exclusive is ok too\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def mag_thresh(image, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # Calculate gradient magnitude\n",
    "    # Apply threshold\n",
    "    sobelx = cv2.Sobel(image, cv2.CV_64F,1,0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(image, cv2.CV_64F,0,1, ksize=sobel_kernel)\n",
    "    mag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    scaled = np.uint8(255*mag/np.max(mag))\n",
    "    mag_binary = np.zeros_like(scaled)\n",
    "    mag_binary[(scaled >= mag_thresh[0]) & (scaled <= mag_thresh[1])] = 1\n",
    "    return mag_binary\n",
    "\n",
    "def dir_threshold(image, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # Calculate gradient direction\n",
    "    # Apply threshold\n",
    "    sobelx = cv2.Sobel(image, cv2.CV_64F,1,0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(image, cv2.CV_64F,0,1, ksize=sobel_kernel)\n",
    "    absx = np.absolute(sobelx)\n",
    "    absy = np.absolute(sobely)\n",
    "    grad_dir = np.arctan2(absy, absx)\n",
    "    dir_binary = np.zeros_like(grad_dir)\n",
    "    dir_binary[(grad_dir >= thresh[0]) & (grad_dir <= thresh[1])] = 1\n",
    "    return dir_binary\n",
    "    \n",
    "def generate_binary_img(image, ksize = 7):\n",
    "    # Choose a Sobel kernel size\n",
    "    # Choose a larger odd number to smooth gradient measurements\n",
    "\n",
    "    # Apply each of the thresholding functions\n",
    "    gradx = abs_sobel_thresh(image, orient='x', sobel_kernel=ksize, thresh=(30, 100))\n",
    "    grady = abs_sobel_thresh(image, orient='y', sobel_kernel=ksize, thresh=(30, 100))\n",
    "    mag_binary = mag_thresh(image, sobel_kernel=ksize, mag_thresh=(30, 100))\n",
    "    dir_binary = dir_threshold(image, sobel_kernel=ksize, thresh=(.7, 1.3))\n",
    "\n",
    "    combined = np.zeros_like(dir_binary)\n",
    "    combined[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))] = 1\n",
    "\n",
    "    return combined\n",
    "```\n",
    "\n",
    "#### Transforming the Image Perspective\n",
    "\n",
    "For image transformation `cv2.warpPerspective()` was used after the source and destination points were designated and a perspective transformation matrix was created via `cv2.getPerspectiveTransform(src,dst)`. \n",
    "\n",
    "The Warp function that was used is below:\n",
    "\n",
    "```python\n",
    "def warp(img):\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    # Four source coordinates for img\n",
    "    xmax = img.shape[1]\n",
    "    ymax = img.shape[0]\n",
    "    offset = 200\n",
    "    \n",
    "    d_top_left = [200,0]\n",
    "    d_top_right = [1000,0]\n",
    "    d_bottom_right = [1000,720]\n",
    "    d_bottom_left = [200,720]\n",
    "    \n",
    "    top_left = [563,471]\n",
    "    top_right = [714,471]\n",
    "    bottom_right = [1090,720]\n",
    "    bottom_left = [221,720]\n",
    "    \n",
    "    src = np.float32(\n",
    "        [top_right,     # top right\n",
    "        bottom_right,   # bottom right\n",
    "        bottom_left,    # bottom left\n",
    "        top_left])      # top left\n",
    "    dst = np.float32(\n",
    "        [[d_top_right],     # top right\n",
    "        [d_bottom_right],   # bottom right\n",
    "        [d_bottom_left],    # bottom left\n",
    "        [d_top_left]])      # top left\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(src,dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst,src)\n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    return warped, Minv\n",
    "```\n",
    "\n",
    "*An example of a warped image can be seen above*\n",
    "\n",
    "#### Identifying Lane-Line Pixels\n",
    "\n",
    "I used a histogram and sliding window approach to identify lane lines. \n",
    "\n",
    "![Lane Histogram][image9]\n",
    "\n",
    "This is the histogram of the warped image. Using this method allows us to easily generalize where the lane lines are assuming the conversion to a binary is good enough quality. \n",
    "\n",
    "Then a sliding window algorithm is applied and a second order polynomial is fit to the detected pixels. The estimated lane line is shown below.\n",
    "\n",
    "![Sliding Window 1][image10]\n",
    "\n",
    "Once we've detected the lanes via the sliding window, we don't need restart the search for the lanes from scratch as we can use the previously discovered sliding windows as starting points to look for the lanes. \n",
    "\n",
    "At this point, using the margin of the windows, we can continue identifying lanes. Lane identification based on previously found lanes is shown below:\n",
    "\n",
    "![Sliding Window 2][image11]\n",
    "\n",
    "\n",
    "#### Calculating Road Curvature and Center of Lane\n",
    "\n",
    "Now that we have two polyfit lines approximating the lane lines we can calculate the center of the lane and the road curvature based on the position of the camera and the projected curve respectively. \n",
    "\n",
    "To get the radius of each lane this equation was used: ![Radius Equation][image12]\n",
    "\n",
    "##### Below is the function used to calculate the radius of the curve and convert the radii from pixels to meters. \n",
    "\n",
    "```python\n",
    "def calc_curve(ploty, left_fitx, right_fitx, left_fit, right_fit):\n",
    "    y_eval = np.max(ploty)\n",
    "    left_curverad = ((1 + (2*left_fit[0]*y_eval + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0])\n",
    "    right_curverad = ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "    \n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    left_fit_cr = np.polyfit(ploty*ym_per_pix, left_fitx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(ploty*ym_per_pix, right_fitx*xm_per_pix, 2)\n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    # Now our radius of curvature is in meters\n",
    "    return left_curverad, right_curverad\n",
    "\n",
    "left_curverad, right_curverad = calc_curve(ploty, left_fitx, right_fitx, left_fit, right_fit)\n",
    "print(left_curverad, 'm', right_curverad, 'm')\n",
    "# Example values: 632.1 m    626.2 m\n",
    "```\n",
    "\n",
    "##### This is the function used to calculate the car position with respect to the center of the lane.\n",
    "This works by taking the midpoints of the two lines that the histogram finds and then calculating how many pixels off center the camera is from the center of the lines.\n",
    "\n",
    "```python\n",
    "def car_position(img):\n",
    "        hist = np.sum(img[img.shape[0]//2:,:], axis=0)\n",
    "        midpt = np.int(hist.shape[0]/2)\n",
    "        camera_position = img.shape[1]/2\n",
    "        left_x_predictions = np.argmax(hist[:midpt])\n",
    "        right_x_predictions = np.argmax(hist[midpt:]) + midpt\n",
    "        lane_center = (right_x_predictions + left_x_predictions)/2\n",
    "        center_offset_pix = abs(camera_position - lane_center)\n",
    "        location_str = \"Vehicle dist. from center: \" + str(center_offset_pix)\n",
    "        return location_str\n",
    "    \n",
    "car_position(img)\n",
    "```\n",
    "\n",
    "#### Final Product\n",
    "\n",
    "The final image result from the pipeline has the projected lane that is based on the detected lane lines and is shown below in green.\n",
    "\n",
    "![Projected Lane][image13]\n",
    "\n",
    "## Video Pipeline\n",
    "\n",
    "\n",
    "\n",
    "## Discussion\n",
    "\n",
    "### 1. Describe your pipeline. As part of the description, explain how you modified the draw_lines() function.\n",
    "\n",
    "My pipeline consisted of 5 steps. First, I converted the images to grayscale, then I .... \n",
    "\n",
    "In order to draw a single line on the left and right lanes, I modified the draw_lines() function by ...\n",
    "\n",
    "If you'd like to include images to show how the pipeline works, here is how to include an image: \n",
    "\n",
    "\n",
    "### 2. Identify potential shortcomings with your current pipeline\n",
    "\n",
    "\n",
    "One potential shortcoming would be what would happen when ... \n",
    "\n",
    "Another shortcoming could be ...\n",
    "\n",
    "\n",
    "### 3. Suggest possible improvements to your pipeline\n",
    "\n",
    "A possible improvement would be to ...\n",
    "\n",
    "Another potential improvement could be to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
